{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('E:\\Girish Documents\\Study\\Data Science\\DataScience_GenAI_Study\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n",
    "\"\"\"\n",
    "#Default format supportable Llama2\n",
    "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this --> \"huggingface-cli login\" in Terminal prompt or conda prompt. \n",
    "##****  USE THE BASE ENV AS THIS NEEDS PyTORCH and it is configured in BASE ENV Only***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78359e3481f54efc87e43c56e44ee94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232128ff571e4afcbc3e9bf8b853321f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.float16,  # Set the desired torch dtype\n",
    "    load_in_8bit=True  # Enable 8-bit quantization\n",
    "    #load_in_8bit_fp32_cpu_offload=True  # Enable CPU offload for 8-bit quantization\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa8182eb613491b9613859da7ed0bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawoo\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sawoo\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d313f768eef8410ca886466750811883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3592f8e80c4643b8c69f83746c6a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85e0f48be454bb392cc2ee3a74475d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537af1757aa0493e8c1ba10d69ba33db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42076a84cf9540c7bd6caecd328804df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawoo\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93846c4907af4c41a9c243df10a51a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dfc3675ebc450c82c630977db3dd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aab7ba7a4d74f9c82b7e7e740741592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d131c36baf434aa098eff398bd3eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155cf79f000048499bc1de9af6268b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "embed_model=LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context=ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x000001B2C7396750>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x000001B2C7396750>, id_func=<function default_id_func at 0x000001B281EA8860>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x000001B4007F1410>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x000001B2C7396750>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=VectorStoreIndex.from_documents(documents,service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x1b400db4690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sawoo\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sawoo\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Attention is all you need is a phrase used in the context of natural language processing and machine learning, particularly in the field of transformer models. It suggests that attention mechanisms, which allow the model to selectively focus on certain parts of the input when processing it, are sufficient for achieving good performance in various NLP tasks.\\n\\nIn the context of the provided PDF, the phrase is used to highlight the importance of attention mechanisms in the Transformer model, which is a type of transformer architecture that uses self-attention mechanisms to process input sequences. The author of the PDF is suggesting that attention is the key to understanding how the Transformer model works and that it is sufficient to focus on attention mechanisms when trying to understand the model.\\n\\nIt is worth noting that the phrase \"attention is all you need\" is an oversimplification, as other components such as feedforward networks and layer normalization are also important in transformer models. However, attention mechanisms are a crucial component of transformer models and are responsible for much of their success in NLP tasks.', source_nodes=[NodeWithScore(node=TextNode(id_='2b8de73b-0703-400a-8e76-0676ada781a1', embedding=None, metadata={'page_label': '14', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c82274de-2c70-4b05-bfad-b4265e6e530a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '14', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}, hash='39d976d5919880e1aec9dff78ae1f3e72994e583579ff5eda52e8fbb4c574795'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6a9c0138-aa07-4bdc-906f-2ac05fca2916', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '13', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}, hash='f210c1f29227b105a496fff30871200f54b45af39b66a2a39c73efb62d4e0e02'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='542b4cd7-1556-486d-830b-2d4a6199ee54', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='da966f9b328bdeb0efb2e1dd46ae40b06b97f7878e2d931ba2ba77cf7039f059')}, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', start_char_idx=0, end_char_idx=814, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5364139298627218), NodeWithScore(node=TextNode(id_='583b87a2-6161-48e8-b990-657c8ce12a07', embedding=None, metadata={'page_label': '5', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1c094620-6fea-4aad-a8e1-aca17eeabdfa', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}, hash='18edb06c8d0810d39f8cd72d3e6c7afd1746646537d598d20a41baa03fb7100f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d63a5757-31f7-4d63-b96f-4ca36fc817a8', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '4', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}, hash='bad2ecb88446cc4d42957ad434ba588a9394a55ae4afe62e3864809c34d9fcff'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2fed6497-1edc-4d8a-87c8-93d130de2d13', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='46c62180d53344b9fe50297bcb1509e8b94a1e66201c9b2b2e8b7edd5c6941b4')}, text='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', start_char_idx=0, end_char_idx=3169, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.48186983566564223)], metadata={'2b8de73b-0703-400a-8e76-0676ada781a1': {'page_label': '14', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}, '583b87a2-6161-48e8-b990-657c8ce12a07': {'page_label': '5', 'file_name': 'AttentionIsAllYouNeed_1706.03762.pdf', 'file_path': 'E:\\\\Girish Documents\\\\Study\\\\Data Science\\\\DataScience_GenAI_Study\\\\GenAI_Project_RAG-LLM-App-Using-LLAMA2-LAAMAIndex\\\\data\\\\AttentionIsAllYouNeed_1706.03762.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-01', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-02-03'}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine.query(\"What is attention is all you need?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
